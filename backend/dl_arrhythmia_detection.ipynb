{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Approach for PPG Arrhythmia Detection\n",
    "\n",
    "This notebook implements a 1D CNN with minimal preprocessing for end-to-end learning.\n",
    "\n",
    "**Signal Characteristics:**\n",
    "- Sampling rate: 100 Hz\n",
    "- Segment duration: 10 seconds\n",
    "- Samples per segment: 1,000 values\n",
    "- Task: Binary classification (Healthy vs Arrhythmic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from deep_learning_pipeline import (\n",
    "    PPGPreprocessor,\n",
    "    DataAugmentation,\n",
    "    build_1d_cnn_model,\n",
    "    create_callbacks,\n",
    "    plot_training_history,\n",
    "    plot_confusion_matrix,\n",
    "    plot_roc_curve\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "print('Libraries imported successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base data directory\n",
    "data_dir = 'data'\n",
    "\n",
    "# Load training data\n",
    "X_train = np.load(os.path.join(data_dir, 'train', 'train_segments.npy'))\n",
    "y_train = np.load(os.path.join(data_dir, 'train', 'train_labels.npy'))\n",
    "train_metadata = pd.read_csv(os.path.join(data_dir, 'train', 'train_metadata.csv'))\n",
    "\n",
    "# Load test data\n",
    "X_test = np.load(os.path.join(data_dir, 'test', 'test_segments.npy'))\n",
    "y_test = np.load(os.path.join(data_dir, 'test', 'test_labels.npy'))\n",
    "test_metadata = pd.read_csv(os.path.join(data_dir, 'test', 'test_metadata.csv'))\n",
    "\n",
    "# Print dataset information\n",
    "print('Dataset Information:')\n",
    "print('=' * 50)\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "print(f'Training labels shape: {y_train.shape}')\n",
    "print(f'Test data shape: {X_test.shape}')\n",
    "print(f'Test labels shape: {y_test.shape}')\n",
    "print(f'\\nClass distribution in training set:')\n",
    "print(pd.Series(y_train).value_counts(normalize=True))\n",
    "print(f'\\nClass distribution in test set:')\n",
    "print(pd.Series(y_test).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualize Raw Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample signals from both classes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 8))\n",
    "\n",
    "# Healthy samples\n",
    "healthy_idx = np.where(y_train == 0)[0]\n",
    "for i in range(2):\n",
    "    sample_idx = healthy_idx[i]\n",
    "    axes[0, i].plot(X_train[sample_idx], linewidth=0.8)\n",
    "    axes[0, i].set_title(f'Healthy Sample {i+1}', fontsize=12, fontweight='bold')\n",
    "    axes[0, i].set_xlabel('Time Steps')\n",
    "    axes[0, i].set_ylabel('Amplitude')\n",
    "    axes[0, i].grid(True, alpha=0.3)\n",
    "\n",
    "# Arrhythmic samples\n",
    "arrhythmic_idx = np.where(y_train == 1)[0]\n",
    "for i in range(2):\n",
    "    sample_idx = arrhythmic_idx[i]\n",
    "    axes[1, i].plot(X_train[sample_idx], linewidth=0.8, color='red')\n",
    "    axes[1, i].set_title(f'Arrhythmic Sample {i+1}', fontsize=12, fontweight='bold')\n",
    "    axes[1, i].set_xlabel('Time Steps')\n",
    "    axes[1, i].set_ylabel('Amplitude')\n",
    "    axes[1, i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocess Data\n",
    "\n",
    "Minimal preprocessing:\n",
    "1. Bandpass filter (0.5-8 Hz) - removes noise outside physiological range\n",
    "2. Z-score normalization - ensures consistent amplitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize preprocessor\n",
    "preprocessor = PPGPreprocessor(sampling_rate=100, lowcut=0.5, highcut=8.0)\n",
    "\n",
    "# Preprocess all signals\n",
    "print('Preprocessing training data...')\n",
    "X_train_processed = preprocessor.preprocess_batch(X_train)\n",
    "\n",
    "print('Preprocessing test data...')\n",
    "X_test_processed = preprocessor.preprocess_batch(X_test)\n",
    "\n",
    "print('Preprocessing complete!')\n",
    "print(f'Preprocessed training data shape: {X_train_processed.shape}')\n",
    "print(f'Preprocessed test data shape: {X_test_processed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare Raw vs Preprocessed Signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare raw and preprocessed signals\n",
    "sample_idx = 0\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 8))\n",
    "\n",
    "# Raw signal\n",
    "axes[0].plot(X_train[sample_idx], linewidth=0.8)\n",
    "axes[0].set_title('Raw PPG Signal', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Time Steps')\n",
    "axes[0].set_ylabel('Amplitude')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Preprocessed signal\n",
    "axes[1].plot(X_train_processed[sample_idx], linewidth=0.8, color='green')\n",
    "axes[1].set_title('Preprocessed PPG Signal (Bandpass + Z-score Normalized)', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Time Steps')\n",
    "axes[1].set_ylabel('Normalized Amplitude')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Prepare Data for CNN\n",
    "\n",
    "Reshape data to CNN input format: (samples, timesteps, channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape for CNN input: (samples, timesteps, channels)\n",
    "X_train_cnn = X_train_processed.reshape(-1, 1000, 1)\n",
    "X_test_cnn = X_test_processed.reshape(-1, 1000, 1)\n",
    "\n",
    "# Create validation split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_cnn, y_train, test_size=0.15, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "print(f'Training set: {X_train_split.shape}')\n",
    "print(f'Validation set: {X_val_split.shape}')\n",
    "print(f'Test set: {X_test_cnn.shape}')\n",
    "print(f'\\nTraining labels distribution:')\n",
    "print(pd.Series(y_train_split).value_counts(normalize=True))\n",
    "print(f'\\nValidation labels distribution:')\n",
    "print(pd.Series(y_val_split).value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Build 1D CNN Model\n",
    "\n",
    "Architecture:\n",
    "- 4 Convolutional blocks with increasing filters (64 → 128 → 256 → 512)\n",
    "- Batch Normalization for stable training\n",
    "- Dropout for regularization\n",
    "- Global Average Pooling to reduce parameters\n",
    "- Dense layers for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build 1D CNN model\n",
    "model = build_1d_cnn_model(input_shape=(1000, 1), num_classes=1)\n",
    "\n",
    "# Display model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Train Model\n",
    "\n",
    "Training strategy:\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Loss: Binary Crossentropy\n",
    "- Callbacks: ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "- Batch size: 64\n",
    "- Max epochs: 100 (with early stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create callbacks\n",
    "callback_list = create_callbacks(model_path='best_arrhythmia_model.keras')\n",
    "\n",
    "# Train model\n",
    "history = model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=callback_list,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Plot Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "from tensorflow import keras\n",
    "best_model = keras.models.load_model('best_arrhythmia_model.keras')\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc, test_auc, test_precision, test_recall = best_model.evaluate(\n",
    "    X_test_cnn, y_test, verbose=1\n",
    ")\n",
    "\n",
    "# Calculate F1-score\n",
    "test_f1 = 2 * (test_precision * test_recall) / (test_precision + test_recall)\n",
    "\n",
    "print(f'\\nTest Results:')\n",
    "print('=' * 50)\n",
    "print(f'  Loss: {test_loss:.4f}')\n",
    "print(f'  Accuracy: {test_acc:.4f}')\n",
    "print(f'  AUC: {test_auc:.4f}')\n",
    "print(f'  Precision: {test_precision:.4f}')\n",
    "print(f'  Recall: {test_recall:.4f}')\n",
    "print(f'  F1-Score: {test_f1:.4f}')\n",
    "print('=' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Predictions and Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_proba = best_model.predict(X_test_cnn).flatten()\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Per-Class Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, f1_score\n",
    "\n",
    "# Detailed classification report\n",
    "print('\\nDetailed Classification Report:')\n",
    "print('=' * 60)\n",
    "print(classification_report(y_test, y_pred, target_names=['Healthy', 'Arrhythmic']))\n",
    "\n",
    "# Per-class F1 scores\n",
    "f1_healthy = f1_score(y_test, y_pred, pos_label=0)\n",
    "f1_arrhythmic = f1_score(y_test, y_pred, pos_label=1)\n",
    "\n",
    "print(f'\\nPer-Class F1 Scores:')\n",
    "print(f'  Healthy: {f1_healthy:.4f}')\n",
    "print(f'  Arrhythmic: {f1_arrhythmic:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Save Complete Pipeline for Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save preprocessor and model together\n",
    "pipeline = {\n",
    "    'preprocessor': preprocessor,\n",
    "    'model_path': 'best_arrhythmia_model.keras',\n",
    "    'sampling_rate': 100,\n",
    "    'signal_length': 1000,\n",
    "    'class_names': ['Healthy', 'Arrhythmic'],\n",
    "    'performance': {\n",
    "        'accuracy': test_acc,\n",
    "        'auc': test_auc,\n",
    "        'precision': test_precision,\n",
    "        'recall': test_recall,\n",
    "        'f1_score': test_f1\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(pipeline, 'arrhythmia_detection_pipeline.pkl')\n",
    "print('Pipeline saved successfully!')\n",
    "print(f'\\nSaved files:')\n",
    "print(f'  - best_arrhythmia_model.keras (trained model)')\n",
    "print(f'  - arrhythmia_detection_pipeline.pkl (complete pipeline)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Test Pipeline on Random Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pipeline\n",
    "pipeline = joblib.load('arrhythmia_detection_pipeline.pkl')\n",
    "model = keras.models.load_model(pipeline['model_path'])\n",
    "\n",
    "# Test on 5 random samples\n",
    "num_samples = 5\n",
    "random_indices = np.random.choice(len(X_test), num_samples, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(num_samples, 1, figsize=(15, 3*num_samples))\n",
    "\n",
    "for i, sample_idx in enumerate(random_indices):\n",
    "    # Get sample\n",
    "    sample_signal = X_test[sample_idx]\n",
    "    \n",
    "    # Preprocess\n",
    "    preprocessed = pipeline['preprocessor'].preprocess(sample_signal)\n",
    "    preprocessed = preprocessed.reshape(1, 1000, 1)\n",
    "    \n",
    "    # Predict\n",
    "    prediction_proba = model.predict(preprocessed, verbose=0)[0][0]\n",
    "    prediction = int(prediction_proba > 0.5)\n",
    "    \n",
    "    # Get true label\n",
    "    true_label = y_test[sample_idx]\n",
    "    \n",
    "    # Plot\n",
    "    ax = axes[i] if num_samples > 1 else axes\n",
    "    ax.plot(sample_signal, linewidth=0.8)\n",
    "    \n",
    "    # Color based on correctness\n",
    "    color = 'green' if prediction == true_label else 'red'\n",
    "    correctness = '✓' if prediction == true_label else '✗'\n",
    "    \n",
    "    ax.set_title(\n",
    "        f'{correctness} Sample {sample_idx} | '\n",
    "        f'True: {pipeline[\"class_names\"][true_label]} | '\n",
    "        f'Predicted: {pipeline[\"class_names\"][prediction]} ({prediction_proba:.2%})',\n",
    "        fontsize=12, fontweight='bold', color=color\n",
    "    )\n",
    "    ax.set_xlabel('Time Steps')\n",
    "    ax.set_ylabel('Amplitude')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Minimal preprocessing (bandpass filter + normalization)\n",
    "2. ✅ 1D CNN architecture for automatic feature learning\n",
    "3. ✅ End-to-end training with callbacks and monitoring\n",
    "4. ✅ Comprehensive evaluation (accuracy, AUC, F1, confusion matrix, ROC)\n",
    "5. ✅ Deployment-ready pipeline\n",
    "\n",
    "**Advantages over feature engineering approach:**\n",
    "- Simpler pipeline (2 preprocessing steps vs 43 features)\n",
    "- Automatic feature learning\n",
    "- Better generalization potential\n",
    "- End-to-end optimization\n",
    "\n",
    "**Next steps:**\n",
    "- Experiment with data augmentation\n",
    "- Try different CNN architectures (ResNet, LSTM)\n",
    "- Ensemble multiple models\n",
    "- Deploy to production"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
